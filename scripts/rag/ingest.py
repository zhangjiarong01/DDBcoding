import os
# --- æ–°å¢è¿™ä¸€è¡Œ ---
# å¼ºåˆ¶ä½¿ç”¨å›½å†…é•œåƒç«™ä¸‹è½½æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ä¸”èƒ½è¿é€š
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
import torch  # å¼•å…¥ torch ç”¨æ¥æ£€æŸ¥ GPU
from llama_index.core import (
    SimpleDirectoryReader, 
    StorageContext, 
    VectorStoreIndex, 
    Settings
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline, IngestionCache
from llama_index.core.storage.docstore import SimpleDocumentStore
from llama_index.vector_stores.lancedb import LanceDBVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

# --- æ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†å¼€å§‹ ---

# 1. æ£€æŸ¥ GPU æ˜¯å¦å¯ç”¨
device_type = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸš€ æ­£åœ¨ä½¿ç”¨çš„è®¡ç®—è®¾å¤‡: {device_type.upper()}")
if device_type == "cuda":
    print(f"   æ˜¾å¡å‹å·: {torch.cuda.get_device_name(0)}")

# 2. è®¾ç½® Embedding æ¨¡å‹
# æ—¢ç„¶æœ‰ A30ï¼Œç›´æ¥ç”¨ BAAI/bge-m3ï¼Œæ•ˆæœæ¯” small å¥½å¾ˆå¤šï¼Œæ”¯æŒå¤šè¯­è¨€å’Œé•¿æ–‡æœ¬
# deviceå‚æ•°å¼ºåˆ¶è®©å®ƒè·‘åœ¨ GPU ä¸Š
Settings.embedding_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-m3", 
    device=device_type 
)
Settings.chunk_size = 512

# --- æ ¸å¿ƒä¿®æ”¹éƒ¨åˆ†ç»“æŸ ---

# å‡†å¤‡å­˜å‚¨è·¯å¾„
# ä¼˜å…ˆåŠ è½½å½“å‰ç›®å½• .env
current_dir = os.path.dirname(os.path.abspath(__file__))
# current_dir is inside scripts/rag/
project_root = os.path.dirname(os.path.dirname(current_dir)) 
# project_root is DDBcoding/

# Load local .env
load_dotenv(os.path.join(current_dir, ".env"))

# Get paths from env or use defaults relative to project root
# .env might contain relative paths provided by user, usually relative to project root (CWD)
env_persist = os.getenv("PERSIST_DIR", "./storage_data")
env_db_uri = os.getenv("DB_URI", "./lancedb_data")
env_docs = os.getenv("DOCS_DIR", "./references")

# Convert to absolute paths based on project_root for safety
PERSIST_DIR = os.path.abspath(os.path.join(project_root, env_persist))
DB_URI = os.path.abspath(os.path.join(project_root, env_db_uri))
DOCS_DIR = os.path.abspath(os.path.join(project_root, env_docs))
MODEL_NAME = "BAAI/bge-m3"

def ingest_documents():
    print(f"å¼€å§‹æ‰«æç›®å½•: {DOCS_DIR}")
    
    vector_store = LanceDBVectorStore(uri=DB_URI, table_name="my_vectors")
    
    try:
        storage_context = StorageContext.from_defaults(persist_dir=PERSIST_DIR)
        docstore = storage_context.docstore
        print("âœ… åŠ è½½äº†å·²æœ‰çš„ Docstore çŠ¶æ€ã€‚")
    except:
        docstore = SimpleDocumentStore()
        storage_context = StorageContext.from_defaults(docstore=docstore)
        print("ğŸ†• åˆ›å»ºäº†æ–°çš„ Docstoreã€‚")

    try:
        # è¯»å–æ–‡ä»¶
        documents = SimpleDirectoryReader(DOCS_DIR, recursive=True, required_exts=[".md"]).load_data()
        if not documents:
            print("âš ï¸ ç›®å½•ä¸­æ²¡æœ‰æ‰¾åˆ° .md æ–‡ä»¶ã€‚")
            return
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å‡ºé”™: {e}")
        return

    pipeline = IngestionPipeline(
        transformations=[
            SentenceSplitter(chunk_size=512, chunk_overlap=50),
            Settings.embedding_model, # è¿™é‡Œä¼šè°ƒç”¨ GPU
        ],
        vector_store=vector_store,
        docstore=docstore,
        cache=IngestionCache(), 
    )

    print("ğŸ”¥ å¼€å§‹è¿è¡Œç®¡é“ (GPU åŠ é€Ÿä¸­)...")
    nodes = pipeline.run(documents=documents, show_progress=True)
    
    storage_context.persist(persist_dir=PERSIST_DIR)
    
    print(f"ğŸ‰ å¤„ç†å®Œæˆï¼æœ¬æ¬¡å¤„ç†äº† {len(nodes)} ä¸ªåˆ†å—ã€‚")

if __name__ == "__main__":
    ingest_documents()
